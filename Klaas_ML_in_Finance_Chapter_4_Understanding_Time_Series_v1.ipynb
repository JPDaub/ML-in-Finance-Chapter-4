{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Klaas - ML in Finance - Chapter 4: Understanding Time Series_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPDaub/ML-in-Finance-Chapter-4/blob/master/Klaas_ML_in_Finance_Chapter_4_Understanding_Time_Series_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR7OlC7sQGPk",
        "colab_type": "text"
      },
      "source": [
        "# Bayesian deep learning\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMcG-f7vSkgi",
        "colab_type": "text"
      },
      "source": [
        "Impulse purchases are often no coincidence as retailers use refined techniques to analyze customer buying behavior. Using this example, Chapter 8 describes and explains how machine learning algorithms are able to search for purchasing patterns of retail customers using barcode scanners, inventory databases and online shopping cards. This search is commonly referred to as market basket analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aagEaJjGZmom",
        "colab_type": "text"
      },
      "source": [
        "## Theoretical basics\n",
        "This part aims to provide the reader with an theoretical understanding of the underlying concepts of market basket analysis.\n",
        "### Understanding association rules\n",
        "The goal of market basket analyses is the generation of association rules which enumerate patterns within the relationship among (purchased) items. An itemset is a group of one or more items in a single transaction and is denoted as follows: \n",
        "> {bread, peanut butter, jelly}.\n",
        "\n",
        "Resulting from this, the denotation of association rules is\n",
        "\n",
        "> {peanut butter,jelly} → {bread}. .\n",
        "\n",
        "The left-hand-side (LHS) is the condition that has to be met in order to trigger the rule and the right-hand-side (RHS) is the expected result of meeting that condition.  \n",
        "Association rules are unsupervised learners which means that there is no need for the algorithm to be trained as it only finds interesting patterns in the dataset. Furthermore, they are used for knowledge discovery in large datasets rather than predictions. The foremost downside of association rules is the absence of an objective way to measure performance rule learner. \n",
        "\n",
        "### The apriori algorithm\n",
        "The datasets used for market basket analyses are almost always large. The number of potential itemsets grows exponentially with the number of features. The apriori algorithm constitutes a solution for this dilemma as the algorithm utilizes a simple prior belief about the properties of frequent itemsets. The apriori property suggests that all subsets of a frequent itemset must also be frequent: If either {motor oil} or {lipstick} or both are considered infrequent the itemset {motor oil, lipstick} is eliminated as well. This approach reduces the number of rules for which the algorithm searches.\n",
        "\n",
        "The algorithm uses statistical measures, namely support, confidence and lift, of an itemset to define a minimum threshold and to locate association rules. The **support** of an itemset answers the question of how frequently an itemset or rule occur in the data. \n",
        "A rule’s **confidence** is the measurement of its predictive power/accuracy and is defined with the following formula: \n",
        "\n",
        "> confidence (X→Y)=(support(X,Y))/(support(X)).\n",
        "\n",
        "Important is that the confidence that X leads to Y is not the same as confidence that Y leads to X. Strong association rules have both high support and high confidence. \n",
        "The metric **lift** measures how much more likely one item or itemset is to be purchased relative to its typical rate of purchase, given that you know another item or itemset has been purchased. The according formula is:\n",
        "\n",
        "> lift(X→Y)=(confidence(X→Y))/(support (Y)).\n",
        "\n",
        "The procedure of the apriori algorithm is as follows:\n",
        "\n",
        "1.   The algorithm identifies all itemsets that meet the minimum support threshold applying multiple iterations with increasing size of itemsets.\n",
        "2.   Then, the algorithm creates rules from these itemsets using those meeting the minimums confidence threshold.\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDUUXEhgzHXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "install.packages('arules', repos='http://cran.rstudio.com/')\n",
        "library(arules)\n",
        "groceries <- read.transactions(\"groceries.csv\", sep = \",\")\n",
        "summary(groceries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwqgXsWiv7_4",
        "colab_type": "text"
      },
      "source": [
        "Bayesian deep learning couples Bayesian approaches with deep learning in order to allow models to express uncertainty. The main idea is the concept of inherent uncertainty present in the model. A simple trick to turn regular deep networks into Bayesian deep networks is to activate dropout during predictions and then make multiple predictions. 20 random values between -5 and 5 are used as X values and the sine function of these values are the Y values. \n",
        "[Code p. 161]\n",
        "The neural network is relatively straightforward. However, Keras does not allow for a dropout layer in the first layer, wherefore, a Dense layer needs to be added which passes the input value through.\n",
        "[Code p. 162]\n",
        "Only a relatively low learning rate is needed to fit this function, so the Keras vanilla stochastic gradient descent optimized is imported in order to set the learning rate there. The model is trained for 10,000 epochs. \n",
        "[Code p. 162 part 2]\n",
        "To test the model over a larger range of values, a test data set with 200 values ranging from -10 to 10 in 0.1 intervals is created.\n",
        "[Code p. 162 part 3]\n",
        "Using keras.backend  the settings are passed to TensorFlow, which runs the operations in the background. The backend is used to set the learning parameter to 1. The Tensor flow will believe that it is in a state of training and will apply dropout. Then, 100 predictions for the test data are made. The result is a probability distribution for the y value at every instance of x.\n",
        "To start the process:\n",
        "1.\tRun the following code [Code p. 162]\n",
        "2.\tObtain the distributions with the following code [Code p. 162 part 2]\n",
        "3.\tCalculate the mean and the standard deviation for the distributions [Code p. 162 part 3]\n",
        "4.\tPlot the model’s predictions with one, two, and four standard deviations: [Code p. 162 part 4] [Graph]\n",
        "The graph shows that the model is confident around the areas where data exists and less confident the further it gets from the data points.\n"
      ]
    }
  ]
}