{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Klaas - ML in Finance - Chapter 4: Understanding Time Series_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fwqgXsWiv7_4"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPDaub/ML-in-Finance-Chapter-4/blob/master/Klaas_ML_in_Finance_Chapter_4_Understanding_Time_Series_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR7OlC7sQGPk",
        "colab_type": "text"
      },
      "source": [
        "# Bayesian deep learning\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWCGItpkVPno",
        "colab_type": "text"
      },
      "source": [
        "Bayesian deep learning couples Bayesian approaches with deep learning in order to allow models to express uncertainty. The main idea is the concept of inherent uncertainty present in the model. A simple trick to turn regular deep networks into Bayesian deep networks is to activate dropout during predictions and then make multiple predictions. 20 random values between -5 and 5 are used as X values and the sine function of these values are the Y values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUP2cmgifkgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X <- runif(20)*10-5\n",
        "Y <- sin (X)\n",
        "Z <- rbind(X,Y)\n",
        "Z\n",
        "length(X)\n",
        "length(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIl1hvWyVamD",
        "colab_type": "text"
      },
      "source": [
        "The neural network is relatively straightforward. However, Keras does not allow for a dropout layer in the first layer, wherefore, a Dense layer needs to be added which passes the input value through."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QRskAp_Vitq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#install.packages('keras', repos='http://cran.rstudio.com/')\n",
        "library(keras)\n",
        "#install.packages('tensorflow', repos='http://cran.rstudio.com/')\n",
        "library(tensorflow)\n",
        "#install.packages('tidyverse', repos='http://cran.rstudio.com/')\n",
        "library(tidyverse)\n",
        "model <- keras_model_sequential()\n",
        "model %>% \n",
        "  layer_dense(1) %>% \n",
        "  \n",
        "  layer_dropout(rate = 0.1) %>%\n",
        "  layer_dense(units = 20, activation = 'relu') %>%\n",
        "  layer_dropout(rate = 0.1) %>%\n",
        "  layer_dense(units = 20, activation = 'relu') %>%\n",
        "  layer_dropout(rate = 0.1) %>%\n",
        "  layer_dense(units = 20, activation = 'sigmoid') %>%\n",
        "  layer_dense(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnuB4XnlVzQp",
        "colab_type": "text"
      },
      "source": [
        "Only a relatively low learning rate is needed to fit this function, so the Keras vanilla stochastic gradient descent optimized is imported in order to set the learning rate there. The model is trained for 10,000 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUPKpjjnV4wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model %>% compile(\n",
        "  optimizer_sgd(lr = 0.1),\n",
        "  loss_mean_squared_error\n",
        ")\n",
        "model %>% fit(\n",
        "  x = X,\n",
        "  y = Y,\n",
        "  epochs = 10000,\n",
        "  batch_size = 10,\n",
        "  verbose = getOption(\"keras.fit_verbose\", default = 0)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WBxbJMpV9ci",
        "colab_type": "text"
      },
      "source": [
        "To test the model over a larger range of values, a test data set with 200 values ranging from -10 to 10 in 0.1 intervals is created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb72n3FMWHBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#install.packages('listarrays', repos='http://cran.rstudio.com/')\n",
        "library(listarrays)\n",
        "X_Test <- seq(-10,10,0.1)\n",
        "X_Test <- expand_dims(X_Test, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KveTTSKWTj7",
        "colab_type": "text"
      },
      "source": [
        "Using keras.backend  the settings are passed to TensorFlow, which runs the operations in the background. The backend is used to set the learning parameter to 1. The Tensor flow will believe that it is in a state of training and will apply dropout. Then, 100 predictions for the test data are made. The result is a probability distribution for the y value at every instance of x.\n",
        "\n",
        "To start the process:\n",
        "\n",
        "1. Run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PJIu8I5WeC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#install.packages('tensorflow', repos='http://cran.rstudio.com/')\n",
        "library(tensorflow)\n",
        "k_clear_session()\n",
        "k_set_learning_phase(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4LHAz66Wqzg",
        "colab_type": "text"
      },
      "source": [
        "2. obtain Obtain the distributions with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KQ3-2HMWvZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "probs <- c()\n",
        "for (i in 1:100){\n",
        "  out <- predict(model, X_Test)\n",
        "  probs <- append(probs, out)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfKZsm5UWzi7",
        "colab_type": "text"
      },
      "source": [
        "3. Calculate the mean and the standard deviation for the distributions:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtR_SbAcW4FW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p <- matrix(probs, ncol = 1, byrow =TRUE)\n",
        "mean <- mean(p)\n",
        "sd <- sd(p)\n",
        "mean\n",
        "sd\n",
        "dim(p)\n",
        "\n",
        "#p <- matrix(probs, ncol = 1, byrow =TRUE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpYkPDEKW5fb",
        "colab_type": "text"
      },
      "source": [
        "4. Plot the model's predictions with one, two, and four standard deviations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULGneulW2L8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#install.packages('ggplot2', repos='http://cran.rstudio.com/')\n",
        "library(ggplot2)\n",
        "X_Test_Plot <- data.frame(X_Test)\n",
        "ggplot(X_Test_Plot,aes(x = X_Test_Plot[,1], y= mean))+\n",
        "  geom_line(aes(color = 'blue'))+\n",
        "  geom_ribbon(aes(ymin=mean-sd*0.5,ymax=mean+sd*0.5),alpha=\"0.25\",fill=\"blue\")+\n",
        "  geom_ribbon(aes(ymin=mean-sd,ymax=mean+sd),alpha=\"0.25\",fill=\"blue\")+\n",
        "  geom_ribbon(aes(ymin=mean-sd*2,ymax=mean+sd*2),alpha=\"0.25\",fill=\"blue\")+\n",
        "  geom_point(aes(X,Y), color='black')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMfRQXixXD8I",
        "colab_type": "text"
      },
      "source": [
        "The graph shows that the model is confident around the areas where data exists and less confident the further it gets from the data points. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwqgXsWiv7_4",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "\n",
        "This chapter provided a broad range of tools for dealing with time series data, insights into how one-dimensional convolution and recurrent architecture works as well as a simple way to train a model to express uncertainty. \n",
        "Recap of the things covered in this chapter:\n",
        "\n",
        "*   Basic data exploration\n",
        "*   Fourier transformation and autocorrelation\n",
        "*   Median forecasting as baseline and sanity check\n",
        "\n",
        "*   Classic prediction models, ARIMA and Kalman filters\n",
        "*   Feature design including data loading mechanisms\n",
        "*   One-dimensional convolutions and variants\n",
        "*   RNNs and LSTMs\n",
        "*   Modeling uncertainty through Bayesian deep learning \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}